import os
from openai import OpenAI
import openpyxl
from openpyxl.styles import Alignment
from datetime import datetime
import time

# Load the API key from environment variable
client = OpenAI(api_key=os.getenv('OPENAI_HMRC_API_KEY'))
total_tokens = 0
prompt_tokens = 0
completion_tokens = 0
max_tokens_per_item = 0
min_tokens_per_item = float('inf')
items_processed = 0

def find_latest_results_folder(base_dir):
    """Find the latest results-lp-YYYY-MM-DD folder in the base directory."""
    results_folders = [folder for folder in os.listdir(base_dir) 
                      if os.path.isdir(os.path.join(base_dir, folder)) 
                      and folder.startswith('results-lp-')]
    
    if not results_folders:
        return None
    
    # Extract dates from folder names and find the latest
    latest_folder = max(results_folders, 
                        key=lambda folder: datetime.strptime(folder.replace('results-lp-', ''), '%Y-%m-%d'))
    
    return os.path.join(base_dir, latest_folder)

# Start timing
start_time = time.time()

# Base directory
base_dir = "/Users/hannahmoutran/Library/CloudStorage/Box-Box/ai-music-metadata-project"

# Find the latest results folder
results_folder = find_latest_results_folder(base_dir)
if not results_folder:
    print("No results folder found! Running the first script is required before this one.")
    exit()

print(f"Using results folder: {results_folder}")

# Find the most recent step2 file in the results folder
step2_files = [f for f in os.listdir(results_folder) 
              if f.startswith('ai-music-step-2-9-scans-lp-4o-') and f.endswith('.xlsx')]
              
if not step2_files:
    print(f"No step2 files found in {results_folder}!")
    exit()

latest_file = max(step2_files)  
workbook_path = os.path.join(results_folder, latest_file)
print(f"Processing file: {workbook_path}")

# Load the workbook and select the active worksheet
wb = openpyxl.load_workbook(workbook_path)
sheet = wb.active

# Define the columns
METADATA_COLUMN = 'E'
OCLC_RESULTS_COLUMN = 'G'
RESULT_COLUMN = 'H'  
CONFIDENCE_SCORE_COLUMN = 'I'  
EXPLANATION_COLUMN = 'J'  

# Add headers for the new columns
sheet[f'{RESULT_COLUMN}1'] = 'LLM-Assessed Correct OCLC #'
sheet[f'{CONFIDENCE_SCORE_COLUMN}1'] = 'LLM Confidence Score'
sheet[f'{EXPLANATION_COLUMN}1'] = 'LLM Explanation'

# Set column widths
sheet.column_dimensions[RESULT_COLUMN].width = 22
sheet.column_dimensions[CONFIDENCE_SCORE_COLUMN].width = 18
sheet.column_dimensions[EXPLANATION_COLUMN].width = 70

# Loop through the rows in the spreadsheet
for row in range(2, sheet.max_row + 1):  # Row 1 is the header
    metadata = sheet[f'{METADATA_COLUMN}{row}'].value
    oclc_results = sheet[f'{OCLC_RESULTS_COLUMN}{row}'].value

    if metadata and oclc_results and oclc_results != "No matching records found":
        items_processed += 1
        # Prepare the prompt for GPT-4
        prompt = (
    f'''Analyze the following OCLC results based on the given metadata and determine which result is most likely correct. 

**Important Instructions**:
1. **Confidence Score**: 0% indicates no confidence, and 100% indicates high confidence that we have found the correct OCLC number.  80% or below indicates that a result should be reviewed by a cataloger. 
2. **Think Critically**: The metadata was generated by an LLM analyzing the back and front cover images of the item. If the metadata is missing certain details (e.g., year, place), assume that this information was not present in the images rather than considering the OCLC record incorrect because it contains that information.
3. **Critical Fields**:
   - Title, artist/performer, publisher, and languages present in the images are very important factors.
   - If the title is in multiple formats (e.g., Romanized, original script), consider both equally unless the context strongly favors one. **Titles in non-Latin scripts that match the metadata in meaning or transliteration should also be considered equivalent, as long as other key fields align.**
   - Physical Details: It is critical that the physical characteristics (e.g., number of discs, disc size) match what is written in the metadata. Try to choose records that match physical characteristics, but if nothing is available that matches other criteria, change the confidence score accordingly.  If the OCLC record indicates multiple discs while the metadata shows a single LP (or vice versa), deduct 20 points from the confidence score.  If, after accounting for unit variations, the OCLC record indicates a different disc size (e.g., 10-inch vs. 12-inch) than the metadata, deduct 20 points from the confidence score.
4. **Critical Mismatches**: 
   - If one record has a critical physical mismatch (e.g., incorrect disc count or size) that another record does not, select the record with the correct physical detailsâ€”even if otherwise its score is slightly lower.
5. **Track Listings and Notes**:
   - If available, compare track listings, contents, or notes in the OCLC records to the metadata. These can provide additional context for matching, though it is not essential.
6. **Avoid Cognitive Bias**:
   - Explicitly compare all records and take the time to thoroughly evaluate all options.

**Format for Response**:s
- Your response **must** follow this format exactly:
  1. OCLC number: [number]
  2. Confidence score: [%]
  3. Explanation: [Short explanation of reasoning behind confidence score.  Example: Things that align: title, subtitle, primary contributor, publisher, format.  Things that don't align: publication date, which is unspecified in the metadata (minor deduction for this omission, as it might not be visible on the album cover)]

**Metadata**: {metadata}

**OCLC Results**: {oclc_results}
'''
)

        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Read through the metadata and OCLC results, and determine which OCLC number is most likely. Consider the context of the metadata and the details in the OCLC records."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.5
            )

            # Update token counts
            item_total_tokens = response.usage.total_tokens
            total_tokens += item_total_tokens
            prompt_tokens += response.usage.prompt_tokens
            completion_tokens += response.usage.completion_tokens
            
            # Update min/max tokens
            max_tokens_per_item = max(max_tokens_per_item, item_total_tokens)
            min_tokens_per_item = min(min_tokens_per_item, item_total_tokens)

            # Print detailed token information for this row
            print(f"\nToken details for row {row}:")
            print(f"  Prompt tokens: {response.usage.prompt_tokens}")
            print(f"  Completion tokens: {response.usage.completion_tokens}")
            print(f"  Total tokens: {item_total_tokens}")
            print(f"  Average tokens per item so far: {total_tokens / items_processed:.2f}")

            # Get the response text
            analysis_result = response.choices[0].message.content.strip()

            try:
                # Split the analysis_result into lines for easier parsing
                lines = analysis_result.splitlines()

                # Initialize placeholders
                oclc_number = "Unknown"
                confidence_score = "Unknown"
                explanation = "No explanation provided"

                # Parse each line
                for line in lines:
                    if line.startswith("1. OCLC number:"):
                        oclc_number = line.replace("1. OCLC number:", "").strip()
                    elif line.startswith("2. Confidence score:"):
                        confidence_score = line.replace("2. Confidence score:", "").replace("%", "").strip()
                    elif line.startswith("3. Explanation:"):
                        explanation = line.replace("3. Explanation:", "").strip()

                # Debugging: Ensure extracted values are correct
                print(f"Debug: Row {row} -> OCLC: {oclc_number}, Confidence: {confidence_score}, Explanation: {explanation}")

            except Exception as e:
                print(f"Error parsing result for row {row}: {e}")
                print(f"Debug: analysis_result = {analysis_result}")  # Debugging output
                oclc_number = "Unknown"
                confidence_score = "Unknown"
                explanation = "Parsing error"

            # Write the result, confidence score, and explanation back to the spreadsheet
            result_cell = sheet[f'{RESULT_COLUMN}{row}']
            confidence_cell = sheet[f'{CONFIDENCE_SCORE_COLUMN}{row}']
            explanation_cell = sheet[f'{EXPLANATION_COLUMN}{row}']

            result_cell.value = int(oclc_number) if oclc_number.isdigit() else oclc_number
            confidence_cell.value = int(confidence_score) if confidence_score.isdigit() else confidence_score
            explanation_cell.value = explanation

            # Add text wrapping for the new columns
            result_cell.alignment = Alignment(wrap_text=True)
            confidence_cell.alignment = Alignment(wrap_text=True)
            explanation_cell.alignment = Alignment(wrap_text=True)

            print(f"Processed row {row}/{sheet.max_row}")

        except Exception as e:
            print(f"Error processing row {row}: {e}")

# Calculate execution time
end_time = time.time()
execution_time = end_time - start_time

# Save the updated workbook to the results folder
current_date = datetime.now().strftime("%Y-%m-%d")
output_file = f"ai-music-step-3-9-scans-lp-4o-{current_date}.xlsx"
full_output_path = os.path.join(results_folder, output_file)
    
wb.save(full_output_path)

# Print final statistics
print("\nFinal Token Statistics:")
print(f"Total tokens used: {total_tokens:,}")
print(f"Total prompt tokens: {prompt_tokens:,}")
print(f"Total completion tokens: {completion_tokens:,}")
print(f"Average tokens per item: {total_tokens / items_processed:.2f}")
print(f"Maximum tokens for a single item: {max_tokens_per_item:,}")
print(f"Minimum tokens for a single item: {min_tokens_per_item:,}")
print(f"Total items processed: {items_processed}")

print(f"\nExecution Summary:")
print(f"Results saved to {full_output_path}")
print(f"Total execution time: {execution_time:.2f} seconds")
print(f"Average time per item: {execution_time / items_processed:.2f} seconds")